{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b77a7e0",
   "metadata": {},
   "source": [
    "# Model Checking\n",
    "\n",
    "After running an MCMC simulation, `sample` returns an object (as of PyMC 3.9, an `InferenceData` object) containing the samples for all the stochastic and deterministic random variables. The final step in Bayesian computation is model checking, in order to ensure that inferences derived from your sample are valid. \n",
    "\n",
    "There are **two components** to model checking:\n",
    "\n",
    "1. Convergence diagnostics\n",
    "2. Goodness of fit\n",
    "\n",
    "Convergence diagnostics are intended to detect **lack of convergence** in the Markov chain Monte Carlo sample; it is used to ensure that you have not halted your sampling too early. However, a converged model is not guaranteed to be a good model. \n",
    "\n",
    "The second component of model checking, goodness of fit, is used to check the **internal validity** of the model, by comparing predictions from the model to the data used to fit the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad18c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "import pymc3 as pm\n",
    "import theano.tensor as tt\n",
    "import arviz as az\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "RANDOM_SEED = 20090425"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e109c8",
   "metadata": {},
   "source": [
    "## Convergence Diagnostics\n",
    "\n",
    "There are a handful of easy-to-use methods for checking convergence. Since you cannot prove convergence, but only show lack of convergence, there is no single method that is foolproof. So, its best to look at a suite of diagnostics together. \n",
    "\n",
    "We will cover the canonical set of checks:\n",
    "\n",
    "- Traceplot\n",
    "- Divergences\n",
    "- R-hat\n",
    "- Effective Sample Size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b9d56f",
   "metadata": {},
   "source": [
    "## Traceplot \n",
    "\n",
    "This is a simple plot that is a good quick check to make sure nothing is obviously wrong, and is usually the first diagnostic step you will take. You've seen these already: just the time series of samples for an individual variable.\n",
    "\n",
    "Let's run the PKU model again as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b6c587",
   "metadata": {},
   "outputs": [],
   "source": [
    "pku_data = pd.read_csv('../data/pku_data.csv')\n",
    "\n",
    "unique_papers = set(pku_data['Paper ID'])\n",
    "paper_map = {p:i for i,p in enumerate(unique_papers)}\n",
    "paper_id = pku_data['Paper ID'].replace(paper_map)\n",
    "phe_std = ((pku_data.Phe - pku_data.Phe.mean()) / pku_data.Phe.std()).values\n",
    "iq = pku_data['IQ'].values\n",
    "concurrent_measurement = pku_data['Concurrent'].values\n",
    "\n",
    "with pm.Model() as pku_model:\n",
    "\n",
    "    μ_int = pm.Normal('μ_int', mu=100, sigma=1e3)\n",
    "    σ_int = pm.HalfCauchy('σ_int', beta=5)\n",
    "    β_0 = pm.Normal('β_0', μ_int, sigma=σ_int, shape=len(unique_papers))\n",
    "    β_1 = pm.Normal('β_1', mu=0, sigma=1e3)\n",
    "    β_2 = pm.Normal('β_2', mu=0, sigma=1e3)\n",
    "\n",
    "    μ_iq = β_0[paper_id] + β_1*phe_std + β_2*concurrent_measurement\n",
    "\n",
    "    σ_iq = pm.HalfCauchy('σ_iq', beta=1)\n",
    "    iq_like = pm.Normal('iq_like', mu=μ_iq, sigma=σ_iq, observed=iq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f4bbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pku_model:\n",
    "\n",
    "    pku_trace = pm.sample(500, tune=0, step=pm.Metropolis(), return_inferencedata=True, random_seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bf7947",
   "metadata": {},
   "source": [
    "The `plot_trace` function from ArViZ by default generates a kernel density plot and a trace plot, with a different color for each chain of the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357a7634",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(pku_trace, var_names=['μ_int', 'σ_iq']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2014b1",
   "metadata": {},
   "source": [
    "This sample is deliberately inadequate. Looking at the trace plot, the problems should be apparent.\n",
    "\n",
    "Can you identify the issues, based on what you learned in the previous section?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168b3f26",
   "metadata": {},
   "source": [
    "### Exercise: Take a quiz!\n",
    "\n",
    "[See how well you can identify sampling problems by looking at their traceplots](https://canyon289.github.io/bayesian-model-evaluation/lessonplans/mcmc_basics/#/14)\n",
    "\n",
    "The slides will show you a trace, and you have to guess whether the sampling is from one of:\n",
    "\n",
    "- MCMC with step size too small\n",
    "- MCMC with step size too large\n",
    "- MCMC with adequate step size\n",
    "- Independent samples from distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b772c1",
   "metadata": {},
   "source": [
    "## Divergences\n",
    "\n",
    "As we have seen, Hamiltonian Monte Carlo (and NUTS) performs numerical integration in order to explore the posterior distribution of a model. When the integration goes wrong, it can go dramatically wrong. \n",
    "\n",
    "For example, here are some Hamiltonian trajectories on the distribution of two correlated variables. Can you spot the divergent path?\n",
    "\n",
    "![divering HMC](images/diverging_hmc.png)\n",
    "\n",
    "The reason that this happens is that there may be parts of the posterior which are **hard to explore** for geometric reasons. Two ways of solving divergences are\n",
    "\n",
    "1. **Set a higher \"target accept\" rate**: Similarly (but not the same) as for Metropolis-Hastings, larger integrator steps lead to lower acceptance rates. A higher `target_accept` will generally cause a smaller step size, and more accurate integration.\n",
    "2. **Reparametrize**: If you can write your model in a different way that has the same joint probability density, you might do that. A lot of work is being done to automate this, since it requires careful work, and one goal of a probabilistic programming language is to iterate quickly. See [Hoffmann, Johnson, Tran (2018)](https://arxiv.org/abs/1811.11926), [Gorinova, Moore, Hoffmann (2019)](https://arxiv.org/abs/1906.03028), and there is work on this also in [symbolic pymc](https://github.com/pymc-devs/symbolic-pymc).\n",
    "\n",
    "You should be wary of a trace that contains many divergences (particularly those clustered in particular regions of the parameter space), and give thought to how to fix them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1881e6",
   "metadata": {},
   "source": [
    "### Divergence example\n",
    "\n",
    "The trajectories above are from a famous example of a difficult geometry: Neal's funnel. It is problematic because the geometry is very different in some regions of the state space relative to others. Specifically, for hierarchical models, as the scale parameter changes in size so do the values of the parameters it is constraining. When the variance is close to zero, the parameter space is very constrained relative to the majority of the support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce8f53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neals_funnel(dims=1):\n",
    "    with pm.Model() as funnel:\n",
    "        v = pm.Normal('v', 0, 3)\n",
    "        x_vec = pm.MvNormal('x_vec', mu=tt.zeros(dims), cov=2 * tt.exp(v) * tt.eye(dims), shape=dims)\n",
    "    return funnel\n",
    "\n",
    "with neals_funnel():\n",
    "    funnel_trace = pm.sample(random_seed=RANDOM_SEED, return_inferencedata=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf82bf7",
   "metadata": {},
   "source": [
    "PyMC3 provides us feedback on divergences, including a count and a recommendation on how to address them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e9f753",
   "metadata": {},
   "outputs": [],
   "source": [
    "diverging_ind = funnel_trace['diverging'].nonzero()[0]\n",
    "diverging_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787d699c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax, *_ = pm.plot_joint(trace)\n",
    "ax.plot(trace['v'][diverging_ind], trace['x_vec'][diverging_ind], 'y.');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60843d1",
   "metadata": {},
   "source": [
    "Let's look at an example of this using the radon example from the first section. Specifically, we will run the random-slopes model, which has a hierarchical model for the basement effect or radon measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f700540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import radon data\n",
    "radon_data = pd.read_csv('../data/radon.csv', index_col=0)\n",
    "\n",
    "counties = radon_data.county.unique()\n",
    "n_counties = counties.shape[0]\n",
    "county = radon_data.county_code.values\n",
    "log_radon = radon_data.log_radon.values\n",
    "floor_measure = radon_data.floor.values\n",
    "log_uranium = np.log(radon_data.Uppm.values)\n",
    "county_lookup = dict(zip(counties, np.arange(n_counties)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb436ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as varying_slope:\n",
    "    \n",
    "    # Priors\n",
    "    μ_b = pm.Normal('μ_b', mu=0., sigma=10)\n",
    "    σ_b = pm.HalfCauchy('σ_b', 5)\n",
    "    \n",
    "    # Common intercepts\n",
    "    a = pm.Normal('a', mu=0., sigma=10)\n",
    "    # Random slopes\n",
    "    b = pm.Normal('b', mu=μ_b, sigma=σ_b, shape=n_counties)\n",
    "    \n",
    "    # Model error\n",
    "    σ_y = pm.HalfCauchy('σ_y',5)\n",
    "    \n",
    "    # Expected value\n",
    "    y_hat = a + b[county] * floor_measure\n",
    "    \n",
    "    # Data likelihood\n",
    "    y_like = pm.Normal('y_like', mu=y_hat, sigma=σ_y, observed=log_radon)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffb792f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with varying_slope:\n",
    "    varying_slope_trace = pm.sample(2000, tune=1000, cores=2, random_seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb52bbed",
   "metadata": {},
   "source": [
    "If we examine the traces of the slope variance and any one of the county slopes, we can see a pathology when the group variance gets very small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e546fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=2)\n",
    "axs[0].plot(varying_slope_trace.get_values('σ_b', chains=0), alpha=.5);\n",
    "axs[0].set(ylabel='σ_b');\n",
    "axs[1].plot(varying_slope_trace.get_values('b', chains=0), alpha=.05);\n",
    "axs[1].set(ylabel='b');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ab16c0",
   "metadata": {},
   "source": [
    "Notice that when the chain reaches the lower end of the parameter space for $\\sigma_b$, it appears to get \"stuck\" and the entire sampler, including the random slopes `b`, mixes poorly. \n",
    "\n",
    "Jointly plotting the random effect variance and one of the individual random slopes demonstrates what is going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc313233",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.Series(varying_slope_trace['b'][:, 10], name='slope')\n",
    "y = pd.Series(varying_slope_trace['σ_b'], name='slope group variance')\n",
    "diverging = varying_slope_trace['diverging']\n",
    "\n",
    "jp = sns.jointplot(x, y, ylim=(0, .7), stat_func=None, alpha=0.3)\n",
    "jp.ax_joint.plot(x[diverging], y[diverging], 'yo');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859ea9ed",
   "metadata": {},
   "source": [
    "When the group variance is small, this implies that the individual random slopes are themselves close to the group mean. In itself, this is not a problem, since this is the behavior we expect. However, if the sampler is tuned for the wider (unconstrained) part of the parameter space, it has trouble in the areas of higher curvature. The consequence of this is that the neighborhood close to the lower bound of $\\sigma_b$ is sampled poorly; indeed, in our chain it is not sampled at all below 0.1. The result of this will be biased inference.\n",
    "\n",
    "The `plot_parallel` function in the ArViZ library is a convenient way to identify patterns in divergent traces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95128cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_parallel(varying_slope_trace, var_names=['b'], figsize=(12,5));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64624526",
   "metadata": {},
   "source": [
    "Now that we've spotted the problem, what can we do about it? The best way to deal with this issue is to reparameterize our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5973fd2e",
   "metadata": {},
   "source": [
    "### Solution: Non-centered Parameterization\n",
    "\n",
    "The partial pooling model specified above uses a **centered** parameterization of the slope random effect. That is, the individual county effects are distributed around a county mean, with a spread controlled by the hierarchical standard deviation parameter. \n",
    "\n",
    "Here is the DAG of this centered model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2784cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.model_to_graphviz(varying_slope)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea0ae58",
   "metadata": {},
   "source": [
    "We can remove the issue with sampling geometry by **reparameterizing** our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127b3d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as varying_slope_noncentered:\n",
    "    \n",
    "    # Priors\n",
    "    μ_b = pm.Normal('μ_b', mu=0., sigma=10)\n",
    "    σ_b = pm.HalfCauchy('σ_b', 5)\n",
    "    \n",
    "    # Common intercepts\n",
    "    a = pm.Normal('a', mu=0., sigma=10)\n",
    "    \n",
    "    # Non-centered random slopes\n",
    "    # Centered: b = Normal('b', μ_b, sigma=σ_b, shape=counties)\n",
    "    υ = pm.Normal('υ', mu=0, sigma=1, shape=n_counties)\n",
    "    b = pm.Deterministic(\"b\", μ_b + υ * σ_b)\n",
    "    \n",
    "    # Model error\n",
    "    σ_y =pm.HalfCauchy('σ_y',5)\n",
    "    \n",
    "    # Expected value\n",
    "    y_hat = a + b[county] * floor_measure\n",
    "    \n",
    "    # Data likelihood\n",
    "    y_like = pm.Normal('y_like', mu=y_hat, sigma=σ_y, observed=log_radon)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc6e9df",
   "metadata": {},
   "source": [
    "This is a **non-centered** parameterization. By this, we mean that the random deviates are no longer explicitly modeled as being centered on $\\mu_b$. Instead, they are independent standard normals $\\upsilon$, which are then scaled by the appropriate value of $\\sigma_b$, before being location-transformed by the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46182fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.model_to_graphviz(varying_slope_noncentered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892a38af",
   "metadata": {},
   "source": [
    "This model samples much better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b8153b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with varying_slope_noncentered:\n",
    "    noncentered_trace = pm.sample(2000, tune=1000, cores=2, random_seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b5f400",
   "metadata": {},
   "source": [
    "Notice that the bottlenecks in the traces are gone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ceea18",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=2)\n",
    "axs[0].plot(noncentered_trace.get_values('σ_b', chains=0), alpha=.5);\n",
    "axs[0].set(ylabel='σ_b');\n",
    "axs[1].plot(noncentered_trace.get_values('b', chains=0), alpha=.5);\n",
    "axs[1].set(ylabel='b');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce6d6f0",
   "metadata": {},
   "source": [
    "And, we are now fully exploring the support of the posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1f6a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.Series(noncentered_trace['b'][:, 75], name='slope')\n",
    "y = pd.Series(noncentered_trace['σ_b'], name='slope group variance')\n",
    "\n",
    "sns.jointplot(x, y, ylim=(0, .7), stat_func=None);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28944bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\n",
    "pm.plot_posterior(varying_slope_trace, var_names=['σ_b'], ax=ax1, color='LightSeaGreen')\n",
    "pm.plot_posterior(noncentered_trace, var_names=['σ_b'], ax=ax2, color='LightSeaGreen')\n",
    "ax1.set_title('Centered (top) and non-centered (bottom)')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6684646b",
   "metadata": {},
   "source": [
    "## Potential Scale Reduction: $\\hat{R}$\n",
    "\n",
    "Roughly, $\\hat{R}$ (*R-Hat*, or the *Gelman-Rubin statistic*) is the ratio of between-chain variance to within-chain variance. This diagnostic uses multiple chains to\n",
    "check for lack of convergence, and is based on the notion that if\n",
    "multiple chains have converged, by definition they should appear very\n",
    "similar to one another; if not, one or more of the chains has failed to\n",
    "converge.\n",
    "\n",
    "$\\hat{R}$ uses an analysis of variance approach to\n",
    "assessing convergence. That is, it calculates both the between-chain\n",
    "varaince (B) and within-chain varaince (W), and assesses whether they\n",
    "are different enough to worry about convergence. Assuming $m$ chains,\n",
    "each of length $n$, quantities are calculated by:\n",
    "\n",
    "$$\\begin{align}B &= \\frac{n}{m-1} \\sum_{j=1}^m (\\bar{\\theta}_{.j} - \\bar{\\theta}_{..})^2 \\\\\n",
    "W &= \\frac{1}{m} \\sum_{j=1}^m \\left[ \\frac{1}{n-1} \\sum_{i=1}^n (\\theta_{ij} - \\bar{\\theta}_{.j})^2 \\right]\n",
    "\\end{align}$$\n",
    "\n",
    "for each scalar estimand $\\theta$. Using these values, an estimate of\n",
    "the marginal posterior variance of $\\theta$ can be calculated:\n",
    "\n",
    "$$\\hat{\\text{Var}}(\\theta | y) = \\frac{n-1}{n} W + \\frac{1}{n} B$$\n",
    "\n",
    "Assuming $\\theta$ was initialized to arbitrary starting points in each\n",
    "chain, this quantity will overestimate the true marginal posterior\n",
    "variance. At the same time, $W$ will tend to underestimate the\n",
    "within-chain variance early in the sampling run. However, in the limit\n",
    "as $n \\rightarrow \n",
    "\\infty$, both quantities will converge to the true variance of $\\theta$.\n",
    "In light of this, $\\hat{R}$ monitors convergence using\n",
    "the ratio:\n",
    "\n",
    "$$\\hat{R} = \\sqrt{\\frac{\\hat{\\text{Var}}(\\theta | y)}{W}}$$\n",
    "\n",
    "This is called the **potential scale reduction**, since it is an estimate of\n",
    "the potential reduction in the scale of $\\theta$ as the number of\n",
    "simulations tends to infinity. In practice, we look for values of\n",
    "$\\hat{R}$ close to one (say, less than 1.1) to be confident that a\n",
    "particular estimand has converged. \n",
    "\n",
    "In ArViZ, the `summary` table, or a `plot_forest` with the `r_hat` flag set, will calculate $\\hat{R}$ for each stochastic node in the trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878b192a",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(pku_trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ccd261",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Clearly the model above has not yet converged (we only ran it for 100 iterations without tuning, after all). Try running the `pku_model` for a larger number of iterations, and see when $\\hat{R}$ converges to 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582103bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce73e45",
   "metadata": {},
   "source": [
    "## Effective Sample Size\n",
    "\n",
    "In general, samples drawn from MCMC algorithms will be autocorrelated. Unless the autocorrelation is very severe, this is not a big deal, other than the fact that autocorrelated chains may require longer sampling in order to adequately characterize posterior quantities of interest. The calculation of autocorrelation is performed for each lag $i=1,2,\\ldots,k$ (the correlation at lag 0 is, of course, 1) by: \n",
    "\n",
    "$$\\hat{\\rho}_i = 1 - \\frac{V_i}{2\\hat{\\text{Var}}(\\theta | y)}$$\n",
    "\n",
    "where $\\hat{\\text{Var}}(\\theta | y)$ is the same estimated variance as calculated for the Gelman-Rubin statistic, and $V_i$ is the variogram at lag $i$ for $\\theta$:\n",
    "\n",
    "$$\\text{V}_i = \\frac{1}{m(n-i)}\\sum_{j=1}^m \\sum_{k=i+1}^n (\\theta_{jk} - \\theta_{j(k-i)})^2$$\n",
    "\n",
    "This autocorrelation can be visualized using the `plot_autocorr` function in ArViZ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8deb5dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_autocorr(pku_trace, var_names=['σ_iq', 'μ_int'], combined=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f79891",
   "metadata": {},
   "source": [
    "You can see very severe autocorrelation in `μ_int`, which is not surprising given the trace that we observed earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d87525",
   "metadata": {},
   "source": [
    "The amount of correlation in an MCMC sample influences the **effective sample size** (ESS) of the sample. The ESS estimates how many *independent* draws contain the same amount of information as the *dependent* sample obtained by MCMC sampling.\n",
    "\n",
    "Given a series of samples $x_j$, the empirical mean is\n",
    "\n",
    "$$\n",
    "\\hat{\\mu} = \\frac{1}{n}\\sum_{j=1}^n x_j\n",
    "$$\n",
    "\n",
    "and the variance of the estimate of the empirical mean is \n",
    "\n",
    "$$\n",
    "\\operatorname{Var}(\\hat{\\mu}) = \\frac{\\sigma^2}{n},\n",
    "$$\n",
    "where $\\sigma^2$ is the true variance of the underlying distribution.\n",
    "\n",
    "Then the effective sample size is defined as the denominator that makes this relationship still be true:\n",
    "\n",
    "$$\n",
    "\\operatorname{Var}(\\hat{\\mu}) = \\frac{\\sigma^2}{n_{\\text{eff}}}.\n",
    "$$\n",
    "\n",
    "The effective sample size is estimated using the partial sum:\n",
    "\n",
    "$$\\hat{n}_{eff} = \\frac{mn}{1 + 2\\sum_{i=1}^T \\hat{\\rho}_i}$$\n",
    "\n",
    "where $T$ is the first odd integer such that $\\hat{\\rho}_{T+1} + \\hat{\\rho}_{T+2}$ is negative.\n",
    "\n",
    "The issue here is related to the fact that we are **estimating** the effective sample size from the fit output. Values of $n_{eff} / n_{iter} < 0.001$ indicate a biased estimator, resulting in an overestimate of the true effective sample size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da9ffe5",
   "metadata": {},
   "source": [
    "Vehtari *et al* (2019) recommend an ESS of at least 400 to ensure reliable estimates of variances and autocorrelations. They also suggest running at least 4 chains before calculating any diagnostics.\n",
    "\n",
    "Its important to note that ESS can vary across the quantiles of the MCMC chain being sampled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0ec7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_ess(pku_trace, var_names=['μ_int']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c03de3f",
   "metadata": {},
   "source": [
    "Using ArViZ, we can visualize the evolution of ESS as the MCMC sample accumulates. When the model is converging properly, both lines in this plot should be approximately linear.\n",
    "\n",
    "The standard ESS estimate, which mainly assesses how well the centre of the distribution is resolved, is referred to as **bulk-ESS**. In order to estimate intervals reliably, it is also important to consider the **tail-ESS**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d6dbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_ess(pku_trace, var_names=['μ_int'], kind='evolution');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff28e84",
   "metadata": {},
   "source": [
    "ESS statistics can also be tabulated, by generating a `summary` of the parameters of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3c8e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(pku_trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c960da",
   "metadata": {},
   "source": [
    "It is tempting to want to **thin** the chain to eliminate the autocorrelation (*e.g.* taking every 20th sample from traces with autocorrelation as high as 20), but this is a waste of time. Since thinning deliberately throws out the majority of the samples, no efficiency is gained; you ultimately require more samples to achive a particular desired sample size. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0c0d57",
   "metadata": {},
   "source": [
    "## Bayesian Fraction of Missing Information\n",
    "\n",
    "The Bayesian fraction of missing information (BFMI) is a measure of how hard it is to\n",
    "sample level sets of the posterior at each iteration. Specifically, it quantifies **how well momentum resampling matches the marginal energy distribution**. \n",
    "\n",
    "$$\\text{BFMI} = \\frac{\\mathbb{E}_{\\pi}[\\text{Var}_{\\pi_{E|q}}(E|q)]}{\\text{Var}_{\\pi_{E}}(E)}$$\n",
    "\n",
    "$$\\widehat{\\text{BFMI}} = \\frac{\\sum_{i=1}^N (E_n - E_{n-1})^2}{\\sum_{i=1}^N (E_n - \\bar{E})^2}$$\n",
    "\n",
    "A small value indicates that the adaptation phase of the sampler was unsuccessful, and invoking the central limit theorem may not be valid. It indicates whether the sampler is able to *efficiently* explore the posterior distribution.\n",
    "\n",
    "Though there is not an established rule of thumb for an adequate threshold, values close to one are optimal. Reparameterizing the model is sometimes helpful for improving this statistic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b5e6a3",
   "metadata": {},
   "source": [
    "BFMI calculation is only available in samples that were simulated using HMC or NUTS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304859bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.bfmi(pku_trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81495182",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pku_model:\n",
    "\n",
    "    pku_trace = pm.sample(return_inferencedata=True, random_seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53775763",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.bfmi(pku_trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf60f2b",
   "metadata": {},
   "source": [
    "Another way of diagnosting this phenomenon is by comparing the overall distribution of \n",
    "energy levels with the *change* of energy between successive samples. Ideally, they should be very similar.\n",
    "\n",
    "If the distribution of energy transitions is narrow relative to the marginal energy distribution, this is a sign of inefficient sampling, as many transitions are required to completely explore the posterior. On the other hand, if the energy transition distribution is similar to that of the marginal energy, this is evidence of efficient sampling, resulting in near-independent samples from the posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b261c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_energy(pku_trace);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c223701",
   "metadata": {},
   "source": [
    "## Goodness of Fit\n",
    "\n",
    "As noted at the beginning of this section, convergence diagnostics are only the first step in the evaluation\n",
    "of MCMC model outputs. It is possible for an entirely unsuitable model to converge, so additional steps are needed to ensure that the estimated model adequately fits the data. \n",
    "\n",
    "One intuitive way of evaluating model fit is to compare model predictions with the observations used to fit\n",
    "the model. In other words, the fitted model can be used to simulate data, and the distribution of the simulated data should resemble the distribution of the actual data.\n",
    "\n",
    "Fortunately, simulating data from the model is a natural component of the Bayesian modelling framework. Recall, from the discussion on prediction, the posterior predictive distribution:\n",
    "\n",
    "$$p(\\tilde{y}|y) = \\int p(\\tilde{y}|\\theta) f(\\theta|y) d\\theta$$\n",
    "\n",
    "Here, $\\tilde{y}$ represents some hypothetical new data that would be expected, taking into account the posterior uncertainty in the model parameters. \n",
    "\n",
    "Sampling from the posterior predictive distribution is easy in PyMC3. The `sample_posterior_predictive` function draws posterior predictive samples from all of the observed variables in the model. Consider the PKU model, \n",
    "where IQ is modeled as a Gaussian random variable, which is thought to be influenced by blood Phe levels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca88d9b",
   "metadata": {},
   "source": [
    "The posterior predictive distribution of IQ uses the same functional form as the data likelihood, in this case a normal stochastic. Here is the corresponding sample from the posterior predictive distribution (we typically need very few samples relative to the MCMC sample):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61685abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pku_model:\n",
    "    pku_ppc = pm.sample_posterior_predictive(pku_trace.posterior, samples=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3ce3b8",
   "metadata": {},
   "source": [
    "The degree to which simulated data correspond to observations can be evaluated visually. This allows for a qualitative comparison of model-based replicates and observations. If there is poor fit, the true value of the data may appear in the tails of the histogram of replicated data, while a good fit will tend to show the true data in high-probability regions of the posterior predictive distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d93f71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(10, 4, figsize=(12,15), sharex=True, sharey=True)\n",
    "\n",
    "for ax in axes.ravel():\n",
    "    i = np.random.randint(0, pku_ppc['iq_like'].shape[1])\n",
    "    ax.hist(pku_ppc['iq_like'][:, i], alpha=0.3)\n",
    "    ax.vlines(iq[i], 0, 100)\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb3acdc",
   "metadata": {},
   "source": [
    "A quantitative approach is to calculate quantiles of each observed data point relative to the corresponding distribution of posterior-simulated values. For an adequate fit, there should not be severe peaks in the histogram near zero and one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1a0e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import percentileofscore\n",
    "\n",
    "plt.hist([np.round(percentileofscore(x, y)/100, 2) for x,y in zip(pku_ppc['iq_like'], iq)], bins=25);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4590a4",
   "metadata": {
    "colab_type": "text",
    "id": "8wt9dlVfUk8C"
   },
   "source": [
    "# Model Comparison\n",
    "\n",
    "To demonstrate the use of model comparison criteria, we implement the radon contamination example from the first day's material. \n",
    "\n",
    "Below, we fit a **pooled model**, which assumes a single fixed effect across all counties, and a **hierarchical model** that allows for a random effect that partially pools the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17ccd9c",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Haujd00xUk8Q"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set_context('notebook')\n",
    "\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7782672",
   "metadata": {
    "colab_type": "text",
    "id": "TGlPozDfUk8b"
   },
   "source": [
    "The data include the observed radon levels and associated covariates for 85 counties in Minnesota."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a196ab94",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dDiw0-MEUk8d",
    "outputId": "7704efc3-81de-457b-f712-f80444653df7"
   },
   "outputs": [],
   "source": [
    "radon_data = pd.read_csv('../data/radon.csv', index_col=0)\n",
    "radon_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9276c3",
   "metadata": {
    "colab_type": "text",
    "id": "ynf9LAklUk8k"
   },
   "source": [
    "### Pooled model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2649ae8c",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rDxh8IsTUk8l",
    "outputId": "deb01f80-a2d1-499f-caf4-fad89463d757",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pymc3 import Model, sample, Normal, HalfCauchy, Uniform\n",
    "\n",
    "floor = radon_data.floor.values\n",
    "log_radon = radon_data.log_radon.values\n",
    "\n",
    "with Model() as pooled_model:\n",
    "    \n",
    "    β = Normal('β', 0, sigma=1e5, shape=2)\n",
    "    σ = HalfCauchy('σ', 5)\n",
    "    \n",
    "    θ = β[0] + β[1]*floor\n",
    "    \n",
    "    y = Normal('y', θ, sd=σ, observed=log_radon)\n",
    "    \n",
    "    trace_p = sample(1000, tune=1000, cores=2, random_seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c9f3d9",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jifzp8PMUk8p",
    "outputId": "b8f26a06-ac6a-4ad0-d944-6bc0e43f7236"
   },
   "outputs": [],
   "source": [
    "from arviz import plot_trace\n",
    "\n",
    "plot_trace(trace_p, var_names=['β']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8be93c",
   "metadata": {},
   "source": [
    "### Unpooled model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc1f5ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "counties = radon_data.county.unique()\n",
    "n_counties = counties.shape[0]\n",
    "county = radon_data.county.replace(dict(zip(counties, np.arange(n_counties)))).values\n",
    "\n",
    "with Model() as unpooled_model:\n",
    "    \n",
    "    β0 = Normal('β0', 0, sigma=10, shape=n_counties)\n",
    "    β1 = Normal('β1', 0, sigma=10)\n",
    "    σ = HalfCauchy('σ', 5)\n",
    "    \n",
    "    θ = β0[county] + β1*floor\n",
    "    \n",
    "    y = Normal('y', θ, sigma=σ, observed=log_radon)\n",
    "    \n",
    "    trace_u = sample(1000, tune=2000, cores=2, random_seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f187b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arviz import plot_trace\n",
    "\n",
    "plot_trace(trace_u, var_names=['β1', 'σ']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715466ae",
   "metadata": {
    "colab_type": "text",
    "id": "Lvnb8fwqUk8t"
   },
   "source": [
    "### Hierarchical model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003bcec7",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wGTlcKoTUk8u",
    "outputId": "650665f6-92bc-4bcb-9a08-f63d7b18fba4"
   },
   "outputs": [],
   "source": [
    "mn_counties = radon_data.county.unique()\n",
    "counties = mn_counties.shape[0]\n",
    "floor_measure = radon_data.floor.values\n",
    "                             \n",
    "with Model() as hierarchical_model:\n",
    "    \n",
    "    # Priors\n",
    "    μ_b0 = Normal('μ_b0', mu=0., sigma=0.0001)\n",
    "    σ_b0 = HalfCauchy('σ_b0', 5)\n",
    "    \n",
    "    \n",
    "    # Random intercepts\n",
    "    υ = Normal('υ', mu=0, sigma=1, shape=n_counties)\n",
    "    β_0 = μ_b0 + υ * σ_b0\n",
    "    # Common slope\n",
    "    β_1 = Normal('β_1', mu=0., sigma=1e5)\n",
    "    \n",
    "    # Model error\n",
    "    σ_y = HalfCauchy('σ_y', 5)\n",
    "    \n",
    "    # Expected value\n",
    "    y_hat = β_0[county] + β_1 * floor_measure\n",
    "    \n",
    "    # Data likelihood\n",
    "    y_like = Normal('y_like', mu=y_hat, sd=σ_y, observed=log_radon)\n",
    "    \n",
    "    trace_h = sample(1000, tune=2000, cores=2, random_seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a885281",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "II8krJa9Uk8x",
    "outputId": "08232c89-12af-4fac-b623-a8474e80b9fa"
   },
   "outputs": [],
   "source": [
    "plot_trace(trace_h, var_names=['μ_b0', 'β_1']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac88c6f9",
   "metadata": {
    "colab_type": "text",
    "id": "LuE8sJ-IUk83"
   },
   "source": [
    "## Predictive Information Criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ced5dbd",
   "metadata": {
    "colab_type": "text",
    "id": "Hn4lEdVkUk84"
   },
   "source": [
    "Measures of predictive accuracy are called **information criteria**, and are comprised of the log-predictive density of the data given a point estimate of the fitted model multiplied by −2 (i.e. the deviance):\n",
    "\n",
    "$$−2 \\log[p(y | \\hat{\\theta})]$$\n",
    "\n",
    "As you might expect, the expected accuracy of a fitted model’s predictions of future data will generally be lower than the accuracy of the model’s predictions for observed data, even though the parameters in the model happen to be sampled from the specified prior distribution.\n",
    "\n",
    "Why are we interested in prediction accuracy?\n",
    "\n",
    "1. to quantify the performance of a model\n",
    "2. to perform model selection \n",
    "\n",
    "By model selection, we may not necessarily want to choose one model over another, but we might want to put different models on the same scale. The advantage if information-theoretic measures is that candidate models do not need to be nested; even models with completely different parameterizations can be used to predict the same measurements.\n",
    "\n",
    "Note that when candidate models have the same number of parameters, one can compare their best-fit log predictive densities directly, but when model dimensions differ, one has to make an adjustment for the tendency of a *larger model to fit data better*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc68afb",
   "metadata": {
    "colab_type": "text",
    "id": "5rhrw-ODUk85"
   },
   "source": [
    "One advantage of using predictive information criteria for model comparison is that they allow us to estimate **out-of-sample predictive accuracy** using the data in our sample. All such methods are *approximations* of predictive accuracy, so they are not perfect, but they perform reasonably well.\n",
    "\n",
    "One can naively use the log predictive density for the sample data (within-sample predictive accuracy) as an approximation of the out-of-sample predictive accuracy, but this will almost always result in an overestimate of performance.\n",
    "\n",
    "As is popular in machine learning, **cross-validation** can be used to evaluate predictive accuracy, whereby the dataset is partitioned and each partition is allowed to be used to fit the model and evaluate the fit. However, this method is computationally expensive because it reqiures the same model to be fit to multiple subsets of the data.\n",
    "\n",
    "We will focus here on **adjusted within-sample predictive accuracy**, using a variety of information criteria. The goal here is to get an approximately unbiased estimate of predictive accuracy which are correct in expectation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0596abbe",
   "metadata": {
    "colab_type": "text",
    "id": "Y7i8DlJ1Uk85"
   },
   "source": [
    "### AIC and DIC\n",
    "\n",
    "One approach to model selection is to use an information-theoretic criterion to identify the most appropriate model. Akaike (1973) found a formal relationship between Kullback-Leibler information (a dominant paradigm in information and coding theory) and likelihood theory. **Akaike's Information Criterion (AIC)** is an estimator of expected relative K-L information based on the maximized log-likelihood function, corrected for asymptotic bias.\n",
    "\n",
    "$$\\text{AIC} = −2 \\log(L(\\theta|data)) + 2K$$\n",
    "\n",
    "AIC balances the **fit of the model** (in terms of the likelihood) with the **number of parameters** required to achieve that fit. We can easily calculate AIC from the residual sums of squares as:\n",
    "\n",
    "$$\\text{AIC} = n \\log(\\text{RSS}/n) + 2k$$\n",
    "\n",
    "where $k$ is the number of parameters in the model. Notice that as the number of parameters increase, the residual sum of squares goes down, but the second term (a penalty) increases.\n",
    "\n",
    "A limitation of AIC for Bayesian models is that it cannot be applied to hierarchical models (or any models with random effects), as counting the number of parameters in such models is problematic. A more Bayesian version of AIC is called the **deviance information criterion (DIC)**, and replaces the fixed parameter penalty with an estimate of the effective number of parameters.\n",
    "\n",
    "$$p_{DIC} = 2\\left(\\log p(y | E[\\theta | y]) - E_{post}[\\log p(y|\\theta)] \\right)$$\n",
    "\n",
    "where the second term is an average of $\\theta$ over the posterior distribution:\n",
    "\n",
    "$$\\hat{p}_{DIC} = 2\\left(\\log p(y | E[\\theta | y]) - \\frac{1}{M} \\sum_{j=1}^{M}\\log p(y|\\theta^{(j)}) \\right)$$\n",
    "\n",
    "DIC is computed as:\n",
    "\n",
    "$$\\text{DIC} = -2 \\log p(y | E[\\theta | y]) + 2p_{DIC}$$\n",
    "\n",
    "Though this is an improvement over AIC, DIC is still not fully Bayesian, as it relies on a **point estimate** of the model rather than using the full posterior. As a result, it can be unstable for hierarchical models, sometimes producing estimates of effective number of parameters that is negative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a668341",
   "metadata": {
    "colab_type": "text",
    "id": "5s2IqN0XUk86"
   },
   "source": [
    "### Widely-applicable Information Criterion (WAIC)\n",
    "\n",
    "WAIC (Watanabe 2010) is a fully Bayesian criterion for estimating out-of-sample expectation, using the **log pointwise posterior predictive density (LPPD)** and correcting for the effective number of parameters to adjust for overfitting.\n",
    "\n",
    "The computed log pointwise predictive density is:\n",
    "\n",
    "$$lppd_{comp} = \\sum_{i=1}^N \\log \\left(\\frac{1}{M} \\sum_{j=1}^M p(y_i | \\theta^{(j)}) \\right)$$\n",
    "\n",
    "The complexity adjustment here is as follows:\n",
    "\n",
    "$$p_{WAIC} = 2\\sum_{i=1}^N \\left[ \\log \\left(\\frac{1}{M} \\sum_{j=1}^M p(y_i | \\theta^{(j)})\\right)  - \\frac{1}{M} \\sum_{j=1}^M \\log p(y_i | \\theta^{(j)})  \\right]$$\n",
    "\n",
    "so WAIC is then:\n",
    "\n",
    "$$\\text{WAIC} = -2(lppd) + 2p_{WAIC}$$\n",
    "\n",
    "The adjustment is an approximation to the **number of unconstrained parameters** in the model (0=fully constrained, 1=no constraints). In this sense, WAIC treats the effective number of paramters as a random variable.\n",
    "\n",
    "WAIC *averages* over the posterior distribution, and therefore is more reliable for a wider range of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919bfebf",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x7NZN7JeUk87",
    "outputId": "2ba70c7c-82e2-4160-992a-0e9295285e88"
   },
   "outputs": [],
   "source": [
    "from arviz import waic\n",
    "\n",
    "pooled_waic = waic(trace_p, pooled_model)\n",
    "    \n",
    "pooled_waic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e35402",
   "metadata": {},
   "outputs": [],
   "source": [
    "unpooled_waic = waic(trace_u, unpooled_model)\n",
    "    \n",
    "unpooled_waic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782ba1d0",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "taRbLB4FUk89",
    "outputId": "dbc9850e-8a99-4aa7-af79-ce8d3a83a45b"
   },
   "outputs": [],
   "source": [
    "hierarchical_waic = waic(trace_h, hierarchical_model)\n",
    "    \n",
    "hierarchical_waic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05aa9855",
   "metadata": {
    "colab_type": "text",
    "id": "WHEg3KRWUk9A"
   },
   "source": [
    "PyMC3 includes two convenience functions to help compare WAIC for different models. The first of this functions is `compare`, this one computes WAIC (or LOO) from a set of traces and models and returns a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c705cb11",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "skwg6Sr2Uk9A",
    "outputId": "6cb9b914-af2b-4857-9382-3aed58b82147"
   },
   "outputs": [],
   "source": [
    "from arviz import compare\n",
    "\n",
    "df_comp_WAIC = compare({'hierarchical':trace_h, 'pooled':trace_p, 'unpooled':trace_u}, ic='waic', seed=RANDOM_SEED)\n",
    "df_comp_WAIC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ba46d5",
   "metadata": {
    "colab_type": "text",
    "id": "x33jOJ5zUk9D"
   },
   "source": [
    "We have many columns so let's check one by one the meaning of them:\n",
    "\n",
    "1. The first column is the **rank order** of the models; zero is best.\n",
    "\n",
    "2. The second column contains the **values of WAIC**. The DataFrame is always sorted from lowest to highest WAIC. The index reflects the order in which the models are passed to this function.\n",
    "\n",
    "3. The third column is the estimated **effective number of parameters**. In general, models with more parameters will be more flexible to fit data and at the same time could also lead to overfitting. Thus we can interpret pWAIC as a penalization term, intuitively we can also interpret it as measure of how flexible each model is in fitting the data.\n",
    "\n",
    "4. The fourth column is the **difference between the value of WAIC for the top-ranked model and the value of WAIC for each model**. For this reason we will always get a value of 0 for the first model. \n",
    "\n",
    "5. The fifth column contains **model weights**. Sometimes when comparing models, we do not want to select the \"best\" model, instead we want to perform predictions by averaging along all the models (or at least several models). Ideally we would like to perform a weighted average, giving more weight to the model that seems to explain/predict the data better. There are many approaches to perform this task, one of them is to use Akaike weights based on the values of WAIC for each model. These weights can be loosely interpreted as the probability of each model (among the compared models) given the data. One caveat of this approach is that the weights are based on point estimates of WAIC (i.e. the uncertainty is ignored).\n",
    "\n",
    "6. The sixth column records the **standard error for the WAIC computations**. The standard error can be useful to assess the uncertainty of the WAIC estimates. Nevertheless, caution need to be taken because the estimation of the standard error assumes normality and hence could be problematic when the sample size is low.\n",
    "\n",
    "7. dSE is the **standard error of the difference** in IC between each model and the top-ranked model. It’s always 0 for the top-ranked model. \n",
    "\n",
    "8. The second-last column is a flag for **warnings**. A value of `True` indicates that the computation of WAIC may not be reliable, this warning is based on an empirical determined cutoff value and need to be interpreted with caution. For more details you can read this [paper](https://arxiv.org/abs/1507.04544).\n",
    "\n",
    "9. The last column indicates the **scale** used for the information criterion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929238a5",
   "metadata": {
    "colab_type": "text",
    "id": "3lPxK4asUk9D"
   },
   "source": [
    "The second convenience function takes the output of `compare` and produces a summary plot in the style of the one used in the book [Statistical Rethinking](http://xcelab.net/rm/statistical-rethinking/) by Richard McElreath (check also [this port](https://github.com/aloctavodia/Statistical-Rethinking-with-Python-and-PyMC3) of the examples in the book to PyMC3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b449287",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6I4dtRg7Uk9E",
    "outputId": "daa1bbad-025b-46b6-c620-94c71fcf9645"
   },
   "outputs": [],
   "source": [
    "from arviz import plot_compare\n",
    "\n",
    "ax = plot_compare(df_comp_WAIC);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9fdc0d",
   "metadata": {
    "colab_type": "text",
    "id": "40_RMpCIUk9H"
   },
   "source": [
    "- The empty circle represents the values of WAIC and the black error bars associated with them are the values of the standard deviation of WAIC. \n",
    "- The value of the lowest WAIC is also indicated with a vertical dashed grey line to ease comparison with other WAIC values.\n",
    "- The filled black dots are the in-sample deviance of each model, which for WAIC is  2 pWAIC from the corresponding WAIC value.\n",
    "- For all models except the top-ranked one we also get a triangle indicating the value of the difference of WAIC between that model and the top model and a grey errorbar indicating the standard error of the differences between the top-ranked WAIC and WAIC for each model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b9d75d",
   "metadata": {
    "colab_type": "text",
    "id": "IETlJDt2Uk9H"
   },
   "source": [
    "### Leave-one-out Cross-validation (LOO)\n",
    "\n",
    "LOO cross-validation is another estimate of the **out-of-sample predictive fit**. In cross-validation, the data are repeatedly partitioned into training and holdout sets, iteratively fitting the model with the former and evaluating the fit with the holdout data. \n",
    "\n",
    "The estimate of out-of-sample predictive fit from applying LOO cross-validation to a Bayesian model is:\n",
    "\n",
    "$$lppd_{loo} = \\sum_{i=1}^N \\log p_{post(-i)}(y_i) =  \\sum_{i=1}^N \\log \\left(\\frac{1}{S} \\sum_{s=1}^S p(y_i| \\theta^{(is)})\\right)$$\n",
    "\n",
    "so, each prediction is conditioned on $N-1$ data points, which induces an **underestimation of the predictive fit** for smaller $N$. The resulting estimate of effective samples size is:\n",
    "\n",
    "$$p_{loo} = lppd - lppd_{loo}$$\n",
    "\n",
    "As mentioned, using cross-validation for a Bayesian model, fitting $N$ copies of the model under different subsets of the data is computationally expensive. However, Vehtari *et al.* (2016) introduced an efficient computation of LOO from MCMC sample, which are corrected using **Pareto-smoothed importance sampling (PSIS)** to provide an estimate of point-wise out-of-sample prediction accuracy.\n",
    "\n",
    "This involves estimating the importance sampling LOO predictive distribution\n",
    "\n",
    "$$p(\\tilde{y}_i | y_{-i}) \\approx \\frac{\\sum_{s=1}^S w_i(\\theta^{(s)}) p(\\tilde{y}_i|\\theta^{(s)})}{\\sum_{s=1}^S w_i(\\theta^{(s)})}$$\n",
    "\n",
    "where the importance weights are:\n",
    "\n",
    "$$w_i(\\theta^{(s)}) = \\frac{1}{p(y_i | \\theta^{(s)})} \\propto \\frac{p(\\theta^{(s)}|y_{-i})}{p(\\theta^{(s)}|y)}$$\n",
    "\n",
    "The predictive distribution evaluated at the held-out point is then:\n",
    "\n",
    "$$p(y_i | y_{-i}) \\approx \\frac{1}{\\frac{1}{S} \\sum_{s=1}^S \\frac{1}{p(y_i | \\theta^{(s)})}}$$\n",
    "\n",
    "However, the posterior is likely to have a *smaller variance and thinner tails* than the LOO posteriors, so this approximation induces instability due to the fact that the importance ratios can have high or infinite variance.\n",
    "\n",
    "To deal with this instability, a generalized **Pareto distribution** fit to the upper tail of the distribution of the importance ratios can be used to construct a test for a finite importance ratio variance. If the test suggests the variance is infinite then importance sampling is halted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe85c3d4",
   "metadata": {
    "colab_type": "text",
    "id": "kBllfEkTUk9I"
   },
   "source": [
    "LOO using Pareto-smoothed importance sampling is implemented in ArviZ in the `loo` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e4e150",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "It9zlf2qUk9J",
    "outputId": "c096d54b-52a4-4c20-fb5d-f34d72f00691"
   },
   "outputs": [],
   "source": [
    "from arviz import loo\n",
    "\n",
    "pooled_loo = loo(trace_p, pooled_model)\n",
    "    \n",
    "pooled_loo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907271a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "unpooled_loo = loo(trace_u, unpooled_model)\n",
    "    \n",
    "unpooled_loo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e37bc1",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nzCpazBBUk9L",
    "outputId": "d71658f8-271a-4769-c088-904967d7dd43"
   },
   "outputs": [],
   "source": [
    "hierarchical_loo  = loo(trace_h, hierarchical_model)\n",
    "    \n",
    "hierarchical_loo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3433fb8",
   "metadata": {
    "colab_type": "text",
    "id": "GNVs3i0_Uk9O"
   },
   "source": [
    "We can also use `compare` with LOO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb145c36",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bm0xyAeaUk9O",
    "outputId": "0a39bc1c-cb23-48cd-8087-c1c3c1a51c25"
   },
   "outputs": [],
   "source": [
    "df_comp_LOO = compare({'hierarchical':trace_h, 'pooled':trace_p, 'unpooled':trace_u}, ic='LOO', seed=RANDOM_SEED)\n",
    "df_comp_LOO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2a458e",
   "metadata": {
    "colab_type": "text",
    "id": "NHxu5PIGUk9R"
   },
   "source": [
    "We can also plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090dbafd",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UL0iU713Uk9R",
    "outputId": "05f21123-6c0d-46b8-fb11-bc99b85312b3"
   },
   "outputs": [],
   "source": [
    "plot_compare(df_comp_LOO);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94fe745",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "Though we might expect the hierarchical model to vastly outperform non-hierarchical models, there is little to choose between the hierarchical and unpooled models in this case, given that these two models give similar values of the information criteria. This is more clearly appreciated when we take into account the uncertainty (in terms of standard errors) of WAIC and LOO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05420bb2",
   "metadata": {
    "colab_type": "text",
    "id": "Cmyutcs6Uk9U"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Reference\n",
    "\n",
    "Gelman, A., & Rubin, D. B. (1992). Inference from iterative simulation using multiple sequences. Statistical Science. A Review Journal of the Institute of Mathematical Statistics, 457–472.\n",
    "\n",
    "[Vehtari, Gelman, Simpson, Carpenter, Bürkner (2019)](https://arxiv.org/abs/1903.08008) Rank-normalization, folding, and localization: An improved $\\hat{R}$ for assessing convergence of MCMC\n",
    "\n",
    "[Burnham, K. and Anderson, K. (2013). Model Selection and Multimodel Inference: A Practical Information-Theoretic Approach](https://www.amazon.co.uk/Model-Selection-Multimodel-Inference-Information-Theoretic/dp/1441929738)\n",
    "\n",
    "[Gelman, A., Hwang, J., & Vehtari, A. (2014). Understanding predictive information criteria for Bayesian models. Statistics and Computing, 24(6), 997–1016.](http://doi.org/10.1007/s11222-013-9416-2)\n",
    "\n",
    "[Vehtari, A, Gelman, A, Gabry, J. (2016). Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC. Statistics and Computing](http://link.springer.com/article/10.1007/s11222-016-9696-4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('pymc-dev-py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "82ed77e24ff330c0a44af0675a1564e555d12c9ca379933fc2f5fab7d76686ed"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
